% Institute of Computer Science thesis template
% authors: Sven Laur, Liina Kamm, Tõnu Tamme
% last change Eero Vainikko <eero.vainikko@ut.ee> 12.01.2021
%--
% Compilation instructions:
% 1. Choose main language on line 55-56 (English or Estonian)
% 2. Compile 1-3 times to get refences right
% pdflatex unitartucs-thesis-template
% bibtex unitartucs-thesis-template
%--
% Please use references like this:
% <text> <non-breaking-space> <cite/ref-command> <punctuation>
% This is an example~\cite{example}.

\documentclass[12pt]{article}

% A package for setting layout and margins for your thesis 
\usepackage[a4paper]{geometry}

%%=== A4 page setup ===
%\setlength{\paperwidth}{21.0cm} 
%\setlength{\paperheight}{29.7cm}
%\setlength{\textwidth}{16cm}
%\setlength{\textheight}{25cm}


% When you write in Estonian then you want to use text with right character set
% By default LaTeX does not know what to do with õäöu letters. You have to specify
% a correct input and font encoding. For that you have to Google the Web     
%
% For TexShop under MacOS X. The right lines are 
%\usepackage[applemac]{inputenc}
%\usepackage[T1]{fontenc} %Absolutely critical for *hyphenation* of words with non-ASCII letters.
%
% For Windows and Linux the right magic lines are   
% \usepackage[latin1]{inputenc}
% \usepackage[latin5]{inputenc}
%
\usepackage[utf8]{inputenc} %standard encoding since 2018 (can be commented out?)
\usepackage[T1]{fontenc} %Absolutely critical for *hyphenation* of words with non-ASCII letters.

% Typeset text in Times Roman instead of Computer Modern (EC)
\usepackage{times}

% Suggested packages:
\usepackage{microtype}  %towards typographic perfection...
\usepackage{inconsolata} %nicer font for code listings. (Use \ttfamily for lstinline bastype)


% Use package babel for English or Estonian 
% If you use Estonian make sure that Estonian hyphenation is installed 
% - hypen-estonian or eehyp packages
%
%===Choose the main language in thesis
\usepackage[estonian, english]{babel} %the thesis is in English 
%\usepackage[english, estonian]{babel} %the thesis is in Estonian

% Change Babel document elements 
\addto\captionsestonian{%
  \renewcommand{\refname}{Viidatud kirjandus}%
  \renewcommand{\appendixname}{Lisad}%
}

% If you have problems with Estonian keywords in the bibliography
%\usepackage{biblatex}
%\usepackage[backend=biber]{biblatex}
%\usepackage[style=alphabetic]{biblatex}
%% plain --> \usepackage[style=numeric]{biblatex}
%% abbrv --> \usepackage[style=numeric,firstinits=true]{biblatex}
%% unsrt --> \usepackage[style=numeric,sorting=none]{biblatex}
%% alpha --> \usepackage[style=alphabetic]{biblatex}
%\DefineBibliographyStrings{estonian}{and={ja}}
%\addbibresource{unitartucs-thesis.bib}


% General packages for math in general, theorems and symbols 
% Read ftp://ftp.ams.org/ams/doc/amsmath/short-math-guide.pdf for further information
\usepackage{amsmath} 
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}
% Optional calligraphic fonts    
% \usepackage[mathscr]{eucal}

% Print a dot instead of colon in table or figure captions
\usepackage[labelsep=period]{caption}

% Packages for building tables and tabulars 
\usepackage{array}
\usepackage{tabu}   % Wide lines in tables
\usepackage{xspace} % Non-eatable spaces in macros

% Including graphical images and setting the figure directory
\usepackage{graphicx}
\graphicspath{{figures/}}

% Packages for getting clickable links in PDF file
%\usepackage{hyperref}
\usepackage[hidelinks]{hyperref} %hide red (blue,green) boxes around links
\usepackage[all]{hypcap}


% Packages for defining colourful text together with some colours
\usepackage{color}
\usepackage{xcolor} 
%\definecolor{dkgreen}{rgb}{0,0.6,0}
%\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}


% Standard package for drawing algorithms
% Since the thesis in article format we must define \chapter for
% the package algorithm2e (otherwise obscure errors occur) 
\let\chapter\section
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}

% Fix a  set of keywords which you use inside algorithms
\SetKw{True}{true}
\SetKw{False}{false}
\SetKwData{typeInt}{Int}
\SetKwData{typeRat}{Rat}
\SetKwData{Defined}{Defined}
\SetKwFunction{parseStatement}{parseStatement}


% Nice todo notes
\usepackage{todonotes}

% comments and verbatim text (code)
\usepackage{verbatim}


% Proper way to create coloured code listings
\usepackage{listings}
\lstset{ 
  %language=python,                % the language of the code
  language=C++,
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  %numbers=left,                   % where to put the line-numbers
  %numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
  numberstyle=\tiny\color{gray}, 
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line 
                                   % will be numbered
  numbersep=5pt,                   % how far the line-numbers are from the code
  backgroundcolor=\color{white},   % choose the background color. You must add \usepackage{color}
  showspaces=false,                % show spaces adding particular underscores
  showstringspaces=false,          % underline spaces within strings
  showtabs=false,                  % show tabs within strings adding particular underscores
  frame = lines,
  %frame=single,                   % adds a frame around the code
  rulecolor=\color{black},		   % if not set, the frame-color may be changed on line-breaks within 
                                   % not-black text (e.g. commens (green here))
  tabsize=2,                       % sets default tabsize to 2 spaces
  captionpos=b,                    % sets the caption-position to bottom
  breaklines=true,                 % sets automatic line breaking
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  %title=\lstname,                 % show the filename of files included with \lstinputlisting;
                                   % also try caption instead of title
  keywordstyle=\color{blue},       % keywCurriculumord style
  commentstyle=\color{dkgreen},    % comment style
  stringstyle=\color{mauve},       % string literal style
  escapeinside={\%*}{*)},          % if you want to add a comment within your code
  morekeywords={*,game, fun}       % if you want to add more keywords to the set
}


% Obscure packages to write logic formulae and program semantics
% Unless you do a thesis on program semantics or static code analysis you do not need that
% http://logicmatters.net/resources/ndexamples/proofsty3.html <= writing type rules => use semantic::inference
% ftp://tug.ctan.org/tex-archive/macros/latex/contrib/semantic/semantic.pdf
\usepackage{proof}
\usepackage{semantic} 
\setlength{\inferLineSkip}{4pt}
\def\predicatebegin #1\predicateend{$\Gamma \vdash #1$}

% If you really want to draw figures in LaTeX use packages tikz or pstricks
% However, getting a corresponding illustrations is really painful  


% Define your favorite macros that you use inside the thesis 
% Name followed by non-removable space
\newcommand{\proveit}{ProveIt\xspace}

% Macros that make sure that the math mode is set
\newcommand{\typeF}[1] {\ensuremath{\mathsf{type_{#1}}}\xspace}
\newcommand{\opDiv}{\ensuremath{\backslash \mathsf{div}}\xspace} 

% Nice Todo box
\setlength{\marginparwidth}{2cm}
\newcommand{\TODO}{\todo[inline]}

% A way to define theorems and lemmata
\newtheorem{theorem}{Theorem}



%%% BEGIN DOCUMENT
\begin{document}

%===BEGIN TITLE PAGE
\thispagestyle{empty}
\begin{center}

\large
\iflanguage{english}{%
UNIVERSITY OF TARTU\\
Faculty of Science and Technology\\
Institute of Computer Science\\
Computer Science Curriculum\\
%Software Engineering Curriculum\\
}{%\iflanguage
TARTU ÜLIKOOL\\
Loodus- ja täppisteaduste valdkond\\
Arvutiteaduse instituut\\
Informaatika õppekava\\
}%\iflanguage

%\vspace*{\stretch{5}}
\vspace{25mm}

\Large Chan Wai Tik

\vspace{4mm}

\huge Emergent Theory of Mind (ToM) from Token Merging


%\vspace*{\stretch{7}}
\vspace{20mm}

\Large
\iflanguage{english}{%
%Bachelor's Thesis (9 ECTS)
Master's Thesis (30 ECTS)
}{%\iflanguage
Bakalaureusetöö (9 EAP)
}%\iflanguage

\end{center}

\vspace{2mm}

\begin{flushright}
 {
 \setlength{\extrarowheight}{5pt}
 \begin{tabular}{r l} 
  \sffamily \iflanguage{english}{Supervisor(s)}{Juhendaja(d)}: &                    \sffamily Kallol Roy, Assistant Professor
 \end{tabular}
 }
\end{flushright}

%\vspace*{\stretch{3}}\iflanguage
%\vspace{10mm}

\vfill
\centerline{\large Tartu \the\year}

%===END TITLE PAGE

% If the thesis is printed on both sides of the page then 
% the second page must be must be empty. Comment this out
% if you print only to one side of the page comment this out
%\newpage
%\thispagestyle{empty}    
%\phantom{Text to fill the page}
% END OF EXTRA PAGE WITHOUT NUMBER


%===COMPULSORY INFO PAGE
\newpage

%=== Info in English
\newcommand\EngInfo{{%
\selectlanguage{english}
\noindent\textbf{\large Emergent Theory of Mind (ToM) from Token Merging}

\vspace*{3ex}

\noindent\textbf{Abstract:}

\noindent
To be done

\vspace*{1ex}

\noindent\textbf{Keywords:}\\
\TODO{List of keywords}
%Layout, formatting, template

\vspace*{1ex}

\noindent\textbf{CERCS:}\TODO{CERCS code and name:~\url{https://www.etis.ee/Portal/Classifiers/Details/d3717f7b-bec8-4cd9-8ea4-c89cd56ca46e}}

\vspace*{1ex}
}}%\newcommand\EngInfo


%=== Info in Estonian
\newcommand\EstInfo{{%
\selectlanguage{estonian}
\noindent\textbf{\large Tüübituletus neljandat järku loogikavalemitele}
\vspace*{1ex}

\noindent\textbf{Lühikokkuvõte:} 

%\noindent ...

\TODO{One or two sentences providing a basic introduction to the field, comprehensible to a scientist in
any discipline.}
\TODO{Two to three sentences of
more detailed background, comprehensible to scientists in related disciplines.}
\TODO{One sentence clearly stating the general problem being addressed by this particular
study.}
\TODO{One sentence summarising the main result (with the words ``here we show´´ or their equivalent).}
\TODO{Two or three sentences explaining what
the main result reveals in direct
comparison to what was thought to be the case previously, or how the main result adds to previous knowledge.}
\TODO{One or two sentences to put the results into a more general context.}
\TODO{Two or three sentences to provide a
broader perspective, readily
comprehensible to a scientist in any
discipline, may be included in the first paragraph
if the editor considers that the accessibility of
the paper is significantly enhanced by their inclusion.}

\vspace*{1ex}

\noindent\textbf{Võtmesõnad:}\\
\TODO{List of keywords}
%Layout, formatting, template

\vspace*{1ex}

\noindent\textbf{CERCS:}\TODO{CERCS kood ja nimetus:~\url{https://www.etis.ee/Portal/Classifiers/Details/d3717f7b-bec8-4cd9-8ea4-c89cd56ca46e}}

\vspace*{1ex}
}}%\newcommand\EstInfo


%=== Determine the order of languages on Info page
\iflanguage{english}{\EngInfo}{\EstInfo}
\iflanguage{estonian}{\EngInfo}{\EstInfo}


\newpage
\tableofcontents

% END OF TODO PAGE 


\newpage
\section{Introduction}

The recent development in Large Language Models (LLM) such as ChatCPT has archived huge success and stormed the world  with its capability of solving varieties of tasks solely based on mastery in language training\cite{cite:1}. However the underlining mechanism is still unknown and hard to investigate due to the large parameter space. On the other hand, some research\cite{cite:2} suggested that LLM has developed a sense of the Theory of Mind (ToM) along the way during its language training as a by-product. Those researches drive the main focus of this report on the linkage between the language model and the Theory of Mind.

\subsection{Language} \label{introLanguage}
Language has variate meanings according to different content and setting, as pointed out in\cite{Hauser_2002}, DNA is a universal language that encodes biological information that is shared along all living organisms on earth, but not the case for communication. In the context of communication, human language also got significantly different from other species in its power of expression which is deeply correlated with the property of human language on hierarchy structured(grammar), generative, and most importantly recursive\cite{Hauser_2002}.  The study of language(Linguistics) is integrated with a wide range of scientific areas from mathematics, philosophy, neuroscience, etc\cite{Gallego_2022}. In viewing language from a biological point of view, bio-linguistics treats it as an evolving organism and reshape the study as the internal and external language (I-Language and E-Language)\cite{Gallego_2022}.  I-language is defined to be a mental mind which is intentional, internal, and individual and represents the computational aspect of language while the E-language is the observable language that humans use in communication\cite{Gallego_2022, Hauser_2002}.  
In this formulation, the study of linguistics focuses on inferring the mechanism of  I-language by observing external language generated by internal language and formulating the grammar in daily use. 
The relationship between grammar, I-language, and E-language can be visualized below (fig:\ref{fig:ilanguage}) \cite{Mark_2006}:
\begin{figure} [!h]
\begin{center}
\includegraphics[width=0.4\textwidth]{figures/iLanguage_eLanguage.png}
\caption{relationship between Grammar, I-language and E-language.}
\label{fig:ilanguage}
\end{center}
\end{figure}

One observation of the external language is its property on an infinite set of expressions together with the constraint on limited memory of the brain which suggested that I-language holds an abstract and generative structure behind the scenes and plays a crucial role in determining the interpretation\cite{Tanaka_2019}. From those observations on external language, Chomsky has proposed universal grammar (UG) and minimalist program(MP) to describe and characterize the formulation and property of I-language.\\
UG hypothesized that human language shares a certain degree of fundamental similarities such as general constraints on grammar and common property on features like lexical categories\cite{D_browska_2015}, etc., and this unique grammar follows the three-factor of language design\cite{Chomsky_2005}:
\begin{itemize}
    \item language-independent principles of data processing
    \item structural architecture
    \item computational efficiency
\end{itemize}
Those principles above assert the base of MP. In the framework of MP, it proposes a bare phrase structure(BPS) together with a simple operator - MERGE to construct it.
The MERGE operator is very simple and takes 2 syntactic arguments and combines them to form a new syntactic object:\\
\begin{center} MERGE($\alpha$,$\beta$)= {$K$, {$\alpha$,$\beta$} } \end{center} \\
Here $\alpha$ and $\beta$ are the two arguments and $K$ is the new object often called "label". The most difficult part here is how to determine the label of the new object, detail can be found in \footnote{LIN331 – Syntactic Theory- https://nlacara.github.io/teaching/331S18/331-7-bps.pdf}
Here is a visualization example of BPS and MERGE operation with lexical categories (fig:\ref{fig:bps}):
\begin{figure} [!h]
\begin{center}
\includegraphics[width=0.2\textwidth]{figures/BSP_merge_lexical.png}
\caption{Example on BPS and MERGE (source\: wiki https\:\/\/en.wikipedia.org\/wiki\/Merge_\%28linguistics\%29} 
\label{fig:bps}
\end{center}
\end{figure}
However, this type of structure can generate syntactically correct but semantically meaningless language as shown by Chomsky in his book " Syntactic Structures".

\newpage
\subsection{Theory of Mind (ToM)}
Theory of Mind has a long history and is an active research field along a broad range of science like developmental or social  psychology, etc, and neuroscience. In the simplest explanation, it can be referred to as the ability of humankind to infer either other's or self's feeling, beliefs, and thoughts which summarize as mental state\cite{Byom_2013}. However, the term mental state is hard to define and quantify due to its complex and abstract concept and the difference in focus on research with respect to variate fields, but it is generally accepted that the ToM concept is a composition of cognitive skills to understand the belief and emotions\cite{Beaudoin_2020}, and base on that, experiment and test on cognitive skills can be developed into 3 categories which summarized in the following table(\ref{tab:TomCognitiveTask})\cite{Beaudoin_2020,Byom_2013}: 

\begin{table}[!h]
\begin{center}
    

\begin{tabular}{|l|r|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Mental\\ State\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Cognitive\\ Skill\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Experiment/\\ Test\end{tabular}}           \\ \hline
    \multicolumn{1}{|c|}{\multirow{2}{*}{Belief}}          & Shared world knowledge                                                         & \begin{tabular}[c]{@{}l@{}} $\bullet$ Text-based tasks \\ $\bullet$ Non-verbal picture-based tasks \end{tabular} \\ \cline{2-3} 
\multicolumn{1}{|c|}{}                                 & Interpreting actions                                                           & $\bullet$ False belief tasks                                                                        \\ \hline
Emotion                                                & Perceiving social cues                                                         & $\bullet$ Facial/Vocal emotion recognition                                                          \\ \hline
\end{tabular}
\end{center}
\caption{Mental State and Related Cognitive Tasks}
\label{tab:TomCognitiveTask}
\end{table}
The above table shows that to access the mental state of belief, one can test the target with either context or false belief base understanding. In the context base test, participants are usually given a textual or picture set of social scenarios, and questions are given related to characters of that scenario either on classification(what the character thinks of) or predication(guessing action taken by the character) to access the inferred mental state of participants. However, there are a number of considerations that need to be careful of as those test demand heavily on working memory and level of language skill. \\

The false-belief test is one step further based on the above tasks which include differences in the mental state of participants and the character in the story\cite{Byom_2013}. In this type of experiment, participants are usually given a storyboard that shows a particular event that alters the result but is only known to the participants but not the protagonist of the story. In this way, the participant is asked to predict the action base on the character's belief(false belief: lacking knowledge of the event) and the belief of the participant's mental state(true belief). This test provides a deviation from the reasoning that humans experience daily which is based on true belief\cite{Byom_2013}. An example of a false belief story is provided in fig(\ref{fig:falseBelief}) for reference.
\newpage

\begin{figure} [!h]
\begin{center}
\includegraphics[width=0.4\textwidth]{figures/false_belief_story.jpg}
\caption{False Belief Example \cite{Wimmer_1983}}
\label{fig:falseBelief}
\end{center}
\end{figure}
Social cues are another important phenomenon to observe mental state changes. Those cues are usually referred to as gaze, facial expression, vocal changes, etc{cite}. While gaze cues are more focused on attention, facial/vocal recognition is related to determining the emotional state of others. 



\subsection{Language Model} \label{introLM}
Since the spark of linguistics, researchers try to model language in a scientific fashion by structural (as described in \ref{introLanguage}) or statistical analysis. In the early day of linguistics which solely relies on hard-crafted rules parsing that was purely symbolic\cite{Nadkarni_2011}. When the time came to the digital age, the use of computers largely influenced research, especially in the power to handle huge textual data and the use of machine learning methods which give rise to natural language processing(NLP) for using computers to process human language.

In the domain of NLP, it can be subdivided into 2 main categories, natural language understanding(NLU) and natural language generation(NLG). In the simplest form, NLU can be understood as it performs syntactic and semantic analysis based on the contents in order to classify the meaning of the given text input. NLG on the other hand, is the process of generating text that meets some specific requirement with a given input and condition, for example, text translation. 
 
In order to handle textual data before modeling(NLU or NLG), the data have to go through several numbers of pre-processing  before feeding to any models. Those steps can be summarized in the following\cite{Vijayarani_2015}:
        
        \begin{itemize}
            \item Tokenization
            \item Stop word removal 
            \item Stemming / Lemmatization - 
            \item Embedding - 
        \end{itemize}

\subsubsection{Tokenization}
In English Textual data, it usually comes in paragraph format and needs to break down into atomic levels which  can be done down to individual character levels, but the usual way will be chopping into words as tokens. However, there are problems arise in chopping at the word level which is mostly related to short-form writing style and punctuation, for example: isn't, O'Neil, while it is relatively easy to handle with the case on O'Neil(change to O Neil ), short-form need to be handle in care. It is also reminded that tokenization is language specific where the strategy change with respect to language.

\subsubsection{Stop word Removal}
The tokenization process will generate a huge amount of tokens from the text data, but some of the tokens occur very frequently which may not contribute any meaning to the text itself. Removing those tokens will help in reducing the dimension of word space. There are several methods to determine stop words, for example, pre-defined list, term frequency, inverse document frequency, etc.

\subsubsection{Stemming/Lemmatization}
English words can exist in different forms like initial, initialed, initialing, and initialization which all refer to the root initial. Stemming is the process to identify the root of the token and aims to reduce the inflectional form in order to reduce  the size of tokens. The purpose of lemmatization is the same, but with different methodologies which use vocabulary and morphological analysis of words to apply the transformation while Stemming is more rely on a heuristic process to truncate part of the word.

\subsubsection{Embedding}
Tokens are still in textual format after those processes, however, most language models work on numerical space rather than discrete text space, the embedding process is to transform tokens into numerical form as a final step before input to the model. There are varieties of ways to achieve the purpose, the simplest one will be one-hot encoding which turns each token into a binary array with length equal to the size of tokens. The disadvantage of this is it creates a very high dimension and  sparsity matrix to represent tokens which may impact the model's convergence time and performance. Other's methods include bag of words together with term frequency-inverse document frequency(TF-IDF) to assign a numerical value for tokens. However, the trend changed to use Word2Vec after adopting the neural network which uses the vector representation before the output layer as the token's embedding. The task of the neural network can be either using surrounding context(words) to predict the target word or the opposite way. This way, it heavily reduces the embedding dimension in comparison to the one hot-encoding and uses the surrounding context to assign numerical meaning to tokens.\\
 
During the pre-deep learning era, there is a wide range of modeling such as support vector machine(SVM) that is based on statistical aspects, but the accuracy heavily depends on the data and the prior belief on the kernel selection. After the deep learning era, especially after the invention of the Long-short term memory model (LSTM, an expansion of recurrent neural network RNN) and Transformer (BERT) which change the trend from statistical to auto-regressive model.  The main difference between the two is that RNN-type models are equipped with state space learning\cite{Siegelmann_1995} to enrich the representation of data. \\

In neural network model setup, those pre-processed data are fed into the model in sequential order, and depending on the model's architecture,  the number of tokens (words) input variate, for example, BERT usually will take a predefined maximum input length of the text while LSTM can be done in sliding window which takes a sequence of text at a time. The output will be the classification result base on the input sequence and it learns by calculating the loss between the ground truth label and its result.

\newpage
\section{ToM and Language - Literature Survey} 
\TODO{Mostly related to ToM and language study from developmental study }
\newpage

\section{Language Model - Literature Survey} 

\newpage


\section{Experiment Setting} 

\subsection{Simulate Incomplete Language Learning} \label{incomplearning}

\subsection{Data Set and Pre-processing}
The experiment is conducted on the IMDB movie review data set {cite} for sentiment classification. The whole data set consists of 50,000 textual reviews and a target label indicating  positive and negative feedback on the movie. 15,000 reviews are sampled from the data as the training set and 3,000 for testing data. The following table(\ref{tab:dataset}) summarizes the positive and negative class distribution on both the training and test set.
\begin{table}[!h]
    \centering
        \begin{tabular}{||c|c| c||} 
        \hline
 Train/Test & Number of Review & Percentage(Pos / Neg) \\ 
 \hline\hline
 Train  &  15000  & 50\% / 50\% \\ 
 \hline
  Test & 3000  & 50\% / 50\%  \\ 
 \hline
    \end{tabular}
     \caption{Training, Testing Set}
    \label{tab:dataset}
\end{table}

Those data sets then go through data cleaning(remove html related tags) and pre-processing pipeline as described in \ref{introLM} apart from the last two processes (Stemming/Lemmatization, embedding). \\

Since the experiment is conducted on destroying the language, the part of speech(POS) tags are used as a selector to control which part of the text should work on instead of doing it at the token level. The following table(\ref{tab:poscount}) shows the top 10 POS tags count for the pre-processed data set.

\begin{table}[!h]
\begin{center}
\fontsize{8pt}{8pt}\selectfont
\begin{tabular}{|c|r|c|c|}
\hline
\multicolumn{1}{|l|}{Data Set} & POS tag & Positive review & Negative Review \\ \hline
\multirow{10}{*}{Train}        & NOUN    & 352,846         & 336,466         \\ \cline{2-4} 
                               & VERB    & 165,504         & 167,988         \\ \cline{2-4} 
                               & ADJ     & 140,768         & 133,559         \\ \cline{2-4} 
                               & PROPN   & 86,150          & 60,792          \\ \cline{2-4} 
                               & ADV     & 37,888          & 40,703          \\ \cline{2-4} 
                               & NUM     & 10,036          & 10,977          \\ \cline{2-4} 
                               & ADP     & 4,911           & 5,620           \\ \cline{2-4} 
                               & DET     & 2,429           & 2,300           \\ \cline{2-4} 
                               & INTJ    & 2,021           & 3,435           \\ \cline{2-4} 
                               & X       & 2,008           & 2,292           \\ \hline
\multirow{10}{*}{Test}         & NOUN    & 74,954          & 69,862          \\ \cline{2-4} 
                               & VERB    & 32,464          & 32,254          \\ \cline{2-4} 
                               & ADJ     & 31,073          & 29,371          \\ \cline{2-4} 
                               & PROPN   & 16,473          & 11,732          \\ \cline{2-4} 
                               & ADV     & 7,580           & 7,715           \\ \cline{2-4} 
                               & DET     & 2,069           & 2,005           \\ \cline{2-4} 
                               & NUM     & 1,870           & 2,073           \\ \cline{2-4} 
                               & ADP     & 937             & 1,146           \\ \cline{2-4} 
                               & INTJ    & 359             & 635             \\ \cline{2-4} 
                               & X       & 344             & 433             \\ \hline
\end{tabular}
     \caption{POS tag count}
    \label{tab:poscount}
\end{center}
\end{table}
\subsection{Token Merge Block}
As described in \ref{incomplearning}, one of the methods to simulate incomplete language is by merging specific tokens which is done by function prepareMerge(\ref{alg:prepareMerge}) and Conv1D. There are 3 parameters to define for merging: 
\begin{enumerate}

    \item $\alpha$ the minimum token to merge 
    \item $\beta$ the maximum token to merge
    \item $l$ a random merge list of size equal input sequence length, lower and upper clipped by $\alpha,\beta$ (ex: [2,3,3,2,3,2]) 
\end{enumerate}

After tokens pass through the embedding layer, it will result in an embedding tensor($\mathbb{E}$) of size NxE (N=sequence length, E=embedding dim). This matrix will then transform to a merge tensor($\methbb{M}$) of size NxEx$\beta$ by selecting the number of tokens to merge($l[i]$) with padding zero at the back if the number of tokens is smaller than $\beta$. The following code demonstrated how the merge is prepared before feeding to Conv1D.

\SetKwComment{Comment}{//}{}
\begin{algorithm}
\fontsize{7pt}{7pt}\selectfont
\caption{PrepareMerge}\label{alg:prepareMerge}
\begin{algorithmic}
\Comment{E=embedding dim,N=sequence length} 
\KwData{$l = [2,3,3,2,3,2]$}
\KwData{$\mathbb{E} = Embedding(tokens).reshape(E,-1)$ } 
\KwData{$\mathbb{M} = tensor.zero(N,E,\beta)$}

\Comment{Start from the end} 
$y \gets l.length()-1$\\
$c \gets l.length()$\\
\While{$c \neq 0$}{
$\mathbb{T} = tensor.zero(E,\beta)$ \\
$m \gets l[y]$\\
$s \gets (c-m)$\\
$\mathbb{T} = \mathbb{E}[:,s:c]$\\
$\mathbb{M}[y] =\mathbb{T}$\\
$c \gets c-s$\\
$y \gets y-1$

}
\end{algorithmic}
\end{algorithm}

The merge tensor $\mathbb{M}$ will be used as input to Conv1D(k=2,stride=1) so that each slice of $\mathbb{M}$ is transformed to the size of (E,1). The following figure(\ref{fig:prepareMerge}) visualized the size transformation from embedding until merge:
\begin{figure} [!h]
\begin{center}
\includegraphics[width=0.3\textwidth]{figures/prepareMerge.png}
\caption{prepareMerge: N=7, $l=[2,2,3,3,2,3,2]$}
\label{fig:prepareMerge}
\end{center}
\end{figure}

\subsection{Model Architecture}
The architecture for the model consists of 4 main blocks which are Embedding, Merge, LSTM, and classification. The following table(\ref{tab:modelsettings}) and diagram(\ref{fig:modelArch}) give a summary and visualization of the parameters setting for those blocks:
\begin{table}[!h]
\begin{tabular}{|c|l|}
\hline
Block                   & \multicolumn{1}{c|}{Parameters}                                 \\ \hline
Embedding               & embedding dim=256                                               \\ \hline
Merge(Conv1D)          & kernel=2,stride=1                                               \\ \hline
LSTM                    & input dim=256, hidden dim=256, hidden layer=3, bidirection=True \\ \hline
Classification(Linear) & input dim=256*2 , out dim = 3                                   \\ \hline
\end{tabular}
\caption{Model Settings}
\label{tab:modelsettings}
\end{table}

\begin{figure} [!h]
\begin{center}
\includegraphics[width=0.6\textwidth]{figures/modelDiagram.png}
\caption{Full model Architecture}
\label{fig:modelArch}
\end{center}
\end{figure}

\subsection{Training}
The training is done in an auto-regressive and mini-batch setting. The review will be fed to the model by pre-defined sequence length and sliding window as a sequence of blocks. The hidden and cell state of LSTM is initialized to 0 at the start of the review and both states of the current block are fed back to the model together with the next block of input.



\newpage
\section{Result and Discussion} 



\newpage
\section{How to add figures and pictures to your thesis}


Here are a few examples of how to add figures or pictures to your thesis (see Figures~\ref{fig:fnCompModel}, \ref{fig:game-based_proofs}, \ref{fig:proveit_screenshot}).

Rule: All the figures, tables and extras in the thesis have to be referred to somewhere in the text.


\begin{figure} [ht] %try to place the figure here (next option top of the page) 
\begin{center}
\includegraphics[width=0.8\textwidth]{computational_model_function}
\caption{The title of the Figure.}
\label{fig:fnCompModel}
\end{center}
\end{figure}



\begin{figure} [!ht] %if [h] doesn't work, we can force with !
\begin{center}
\includegraphics[width=\textwidth]{game-based_proofs}
\caption{Refer if the figure is not yours~\cite{kamm12}.}
\label{fig:game-based_proofs}
\end{center}
\end{figure}


\begin{figure} [p]
\begin{center}
\includegraphics[width=\textwidth]{proveit_screenshot}
\caption{Screenshot of \proveit.}
\label{fig:proveit_screenshot}
\end{center}
\end{figure}

Tip: If you add a screenshot then labeling the parts might help make the text more understandable (panel C vs bottom left part), e.g.


\begin{figure} [htbp]
\begin{tabular}{c c}
%
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{LCA_2_solutions}
\end{minipage}
%
&
\begin{minipage}{0.55\textwidth}
\centering
\begin{tabular}{ l | l |}
	Node & Decendants \\ \hline
  1 & 2, 3, 4 \\ \hline
  2 & 3, 4 \\ \hline
  3 & \\ \hline
  4 & \\ \hline
  5 & 3, 4, 6, 7 \\ \hline
  6 & 4 \\ \hline
  7 & 3 \\  \hline
  8 & 3, 4, 5, 6, 7\\ \hline
  9 & 3, 4, 5, 6, 7\\ \hline
\end{tabular}
\end{minipage}
\end{tabular}
%
\caption{Example how to put two figures parallel to each other.}
\label{fig:LCA_2_solutions}
\end{figure}


Example: A screenshot of \proveit can be seen on Figure~\ref{fig:proveit_screenshot}. The user first enters the pseudocode of the initial game in panel B. \proveit also keeps track of all the previous games showing the progress on a graph seen in panel A.

There are two figures side by side on Figure~\ref{fig:LCA_2_solutions}.



\clearpage %if newpage doesn't work
\section{Other Ways to Represent Data}

\subsection{Tables}

\begin{table}[h]
\centering
\caption{Statements in the \proveit language.}
\begin{tabular}{| l | l |}
	\hline
	\bf{Statement} & \bf{Typeset Example} \\
	\hline
	assignment & $a := 5 + b$ \\
	\hline
	uniform choice & $m <- M$ \\
	\hline
	function signature & $f : K \times M -> L$\\
	\hline
\end{tabular}
\label{tab:statements}
\end{table}


\subsection{Lists}

Numbered list example:
\begin{enumerate}
	\item item one; 
	\item item two;
	\item item three.
\end{enumerate} 

\subsection{Math mode}
Example:
\begin{equation}
a + b = c + d
\end{equation}
Aligning:
\begin{align*}
	a &= 5 \\
	b + c &= a \\
	a -2*3 &= 5/4
\end{align*}
Hint: Variables or equations in text are separated with \$ sign, e.g. $a$, $x - y$.

\paragraph{Inference Rules}
\[ 
	\inference[addition]{x : T & y : T}{x + y : T} 
\]
Bigger example:
\[
\inference[assign]{c := a + b & 
	\inference[addG]{a : \typeRat & 
		\inference[var]{b : \typeInt & \typeInt \subseteq \typeRat}{b : \typeRat}
		}{a + b : \typeRat}
	}{c : \typeRat}
\]


\subsection{algorithm2e}

\begin{algorithm} [!h]
	\caption{typeChecking} \label{alg:typeChecking}
	\KwIn{Abstract syntax tree}
	\KwResult{Type checking result; In addition, type table \typeF{type\_G} for global variables, \typeF{game} for the main game and \typeF{fun} for each $fun \in F$}
	\SetKwData{s}{s}
	\BlankLine
	
	\While{something changed in last cycle}{
		\lForEach{global statement \s} {
			\parseStatement{\s, \typeF{type\_G}}\;
		}
		\ForEach{function $fun$} {
		\lForEach{statement \s in $fun$} {
			\parseStatement{\s, \typeF{fun}}\;
		}
		}
		\lForEach{statement \s in game} {
			\parseStatement{\s, \typeF{game}}\;
		}
	}
	%\eIf{error messages were found}{\Return \False\;}{\Return \True\;}
\end{algorithm}

\subsection{Pseudocode}

\begin{figure} [htb]
\begin{lstlisting}
expression
  : NUMBER
  | VARIABLE
  | '+' expression
  | expression '+' expression
  | expression '*' expression
  | function_name '(' parameters ')'
  | '(' expression ')'
\end{lstlisting}
\caption{Grammar of arithmetic expressions.}
\label{fig:parser_exp}
\end{figure}

\subsection{Frame Around Information}

Tip: We can use minipage to create a frame around some important information.
\begin{figure} [h]
\frame{
\begin{minipage}{\textwidth}
\begin{enumerate}
	\item integer division ($\opDiv$) -- only usable between \typeInt types
	\item remainder ($\%$) -- only usable between \typeInt types
\end{enumerate}
\end{minipage}
}
\caption{Arithmetic operations in \proveit revisited.}
\label{fig:aritmOp_revisit}
\end{figure}



\clearpage
\section{Conclusion}

\TODO{what did you do?} 
\TODO{What are the results?}
\TODO{future work?}

\newpage

% BibTeX bibliography
\bibliographystyle{alpha} %plain=[1], alpha=[BGZ09]
\bibliography{Thesis_ref}

\addcontentsline{toc}{section}{\refname}


% Use Biblatex if you have problems with Estonian keywords
%\printbibliography %biblatex


% Use alternative local LaTeX bibliography
\begin{comment}
\begin{thebibliography}{9}
\bibitem{proVerif} 
  Bruno Blanchet. 
  Proverif: Cryptographic protocol verifier in the formal model.
  \url{http://www.proverif.ens.fr/}.
  (checked 15.05.2012)
\bibitem{GameB_1} GameB1
\bibitem{GameB_2} GameB2
\bibitem{certicrypt} certicrypt
\bibitem{kamm12} kamm12
\end{thebibliography}
\end{comment}


\newpage
%\appendix
%\section*{\appendixname}
\iflanguage{english}%
  {\section*{Appendix}
  \addcontentsline{toc}{section}{Appendix}
  }%
  {\section*{Lisad}
  \addcontentsline{toc}{section}{Lisad}}


\section*{I. Glossary}
\addcontentsline{toc}{subsection}{I. Glossary}

\newpage

%=== Licence in English
\newcommand{\licencehint}[2]{\\\hspace*{#1}\textsl(#2)\par}
\newcommand\EngLicence{{%
\selectlanguage{english}
\section*{II. Licence}

\addcontentsline{toc}{subsection}{II. Licence}

\subsection*{Non-exclusive licence to reproduce thesis and make thesis public}

I, \textbf{Alice Cooper}, %author's name
  \licencehint{10mm}{author's name}

\begin{enumerate}
\item
herewith grant the University of Tartu a free permit (non-exclusive licence) to
\par
reproduce, for the purpose of preservation, including for adding to the DSpace digital archives until the expiry of the term of copyright,
\par
\textbf{Type Inference for Fourth Order Logic Formulae}, %
  \licencehint{10mm}{title of thesis}
\par
supervised by Axel Rose and May Flower. %supervisor's name
  \licencehint{10mm}{supervisor's name}
\item
I grant the University of Tartu a permit to make the work specified in p. 1 available to the public via the web environment of the University of Tartu, including via the DSpace digital archives, under the Creative Commons licence CC BY NC ND 3.0, which allows, by giving appropriate credit to the author, to reproduce, distribute the work and communicate it to the public, and prohibits the creation of derivative works and any commercial use of the work until the expiry of the term of copyright.
\item
I am aware of the fact that the author retains the rights specified in p. 1 and 2.
\item
I certify that granting the non-exclusive licence does not infringe other persons' intellectual property rights or rights arising from the personal data protection legislation. 
\end{enumerate}

\noindent
Alice Cooper\\ %author's name
\textbf{\textsl{dd/mm/yyyy}}
}}%\newcommand\EngLicence


%=== Licence in Estonian
\newcommand\EstLicence{{%
\selectlanguage{estonian}
\section*{II. Litsents}

\addcontentsline{toc}{subsection}{II. Litsents}

\subsection*{Lihtlitsents lõputöö reprodutseerimiseks ja üldsusele kättesaadavaks tegemiseks}

Mina, \textbf{Alice Cooper}, %author's name
  \licencehint{10mm}{autori nimi}

\begin{enumerate}
\item
annan Tartu Ülikoolile tasuta loa (lihtlitsentsi) minu loodud teose
\par
\textbf{Tüübituletus neljandat järku loogikavalemitele}, %title of thesis
    \licencehint{10mm}{lõputöö pealkiri}
\par
mille juhendaja(d) on Axel Rose ja May Flower, %supervisor's name(s)
  \licencehint{10mm}{juhendaja nimi}
\par
reprodutseerimiseks eesmärgiga seda säilitada, sealhulgas lisada digitaalarhiivi DSpace kuni autoriõiguse kehtivuse lõppemiseni.
\par
\item
Annan Tartu Ülikoolile loa teha punktis 1 nimetatud teos üldsusele kättesaadavaks Tartu Ülikooli veebikeskkonna, sealhulgas digitaalarhiivi DSpace kaudu Creative Commonsi litsentsiga CC BY NC ND 3.0, mis lubab autorile viidates teost reprodutseerida, levitada ja üldsusele suunata ning keelab luua tuletatud teost ja kasutada teost ärieesmärgil, kuni autoriõiguse kehtivuse lõppemiseni.
\item
Olen teadlik, et punktides 1 ja 2 nimetatud õigused jäävad alles ka autorile.
\item
Kinnitan, et lihtlitsentsi andmisega ei riku ma teiste isikute intellektuaalomandi ega isikuandmete kaitse õigusaktidest tulenevaid õigusi. 
\end{enumerate}

\noindent
Alice Cooper\\ %author's name
\textbf{\textsl{pp.kk.aaaa}}
}}%\newcommand\EstLicence


%===Choose the licence in active language
\iflanguage{english}{\EngLicence}{\EstLicence}


\end{document}

